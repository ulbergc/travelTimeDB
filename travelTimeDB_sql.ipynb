{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This version tries to use almost all SQL (besides python wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# define the directory that has the struct3dp data files\n",
    "modelname='Run_230'\n",
    "rundir='/Users/ulberg/research/MSH/crosson/Runs'\n",
    "datadir='{}/{}/Data'.format(rundir,modelname)\n",
    "\n",
    "# These file names are always associated with an inversion\n",
    "stafile='combined.sta' # the stations that recorded travel times\n",
    "eqobsfile='quakes.obs' # the travel time observations for earthquakes\n",
    "eqlocfile='quakes.loc' # the locations of earthquakes\n",
    "exobsfile='explos.obs' # the travel time observations for explosions\n",
    "exlocfile='explos.loc' # the locations of explosions\n",
    "\n",
    "# read into dataframes\n",
    "sta=pd.read_csv(datadir + '/' + stafile, names=['Station','Longitude','Latitude','Depth'], sep=' ', skipinitialspace=True)\n",
    "obs_eq=pd.read_csv(datadir + '/' + eqobsfile, names=['SourceID','Station','Phase','ArrivalTime','Uncertainty'], sep=' ', skipinitialspace=True)\n",
    "loc_eq=pd.read_csv(datadir + '/' + eqlocfile, names=['SourceID','Longitude','Latitude','Depth','EventTime'], sep=' ', skipinitialspace=True)\n",
    "obs_ex=pd.read_csv(datadir + '/' + exobsfile, names=['SourceID','Station','Phase','ArrivalTime','Uncertainty'], sep=' ', skipinitialspace=True)\n",
    "loc_ex=pd.read_csv(datadir + '/' + exlocfile, names=['SourceID','Longitude','Latitude','Depth','EventTime'], sep=' ', skipinitialspace=True)\n",
    "\n",
    "# read in more files that provide some mapping between other files\n",
    "mapfile='map.map'\n",
    "antUWfile='AntUWreview.txt'\n",
    "\n",
    "mapcols=['dbname','orid','SourceID','method','Longitude-ant','Latitude-ant','Depth-ant','FullTime']\n",
    "antUWcols=['PNSNid','dbname','orid','picker','Latitude-uw','Longitude-uw','Depth-uw']\n",
    "\n",
    "datamap=pd.read_csv(datadir + '/' + mapfile, names=mapcols, sep=' ', skipinitialspace=True)\n",
    "antUW=pd.read_csv('{}/{}/ANT/{}'.format(rundir,modelname,antUWfile), names=antUWcols, sep=' ', skipinitialspace=True, dtype={'orid': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datamap\n",
      "  dbname orid  SourceID      method  Longitude-ant  Latitude-ant  Depth-ant  \\\n",
      "0  SHOTS  151  8000151p  shots2s3dp     -121.55266      46.71370     -0.668   \n",
      "1  SHOTS  152  8000152p  shots2s3dp     -121.77356      46.20077     -1.029   \n",
      "2  SHOTS  155  8000155p  shots2s3dp     -121.99266      46.18471     -1.009   \n",
      "3  SHOTS  156  8000156p  shots2s3dp     -122.44470      46.04884     -0.834   \n",
      "4  SHOTS  164  8000164p  shots2s3dp     -122.13281      46.08131     -0.594   \n",
      "\n",
      "              FullTime  \n",
      "0  20140724T113500.020  \n",
      "1  20140725T050000.020  \n",
      "2  20140725T072000.020  \n",
      "3  20140725T073500.020  \n",
      "4  20140725T085000.020  \n",
      "antUW\n",
      "     PNSNid   dbname  orid picker  Latitude-uw  Longitude-uw  Depth-uw\n",
      "0  60810532  2014_Q3     2   Carl      46.5397     -122.9978       0.0\n",
      "1  60810642  2014_Q3  1394   Carl      45.8663     -122.4492       0.0\n",
      "2  60058633  2014_Q3  1395   Carl      45.5633     -122.6557      15.9\n",
      "3  60814162  2014_Q3  1396   Carl      46.1728     -122.4225       0.0\n",
      "4         0  2014_Q3  1401   Carl      46.3526     -122.4558       1.4\n"
     ]
    }
   ],
   "source": [
    "print('datamap')\n",
    "print(datamap.head())\n",
    "print('antUW')\n",
    "print(antUW.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add data to sqlite database\n",
    "\n",
    "dbdir='/Users/ulberg/Documents/GitHub/travelTimeDB/DB' # make sure this directory is created before running\n",
    "filename=dbdir + '/tt_db_sql.sqlite'\n",
    "\n",
    "# create engine\n",
    "engine=create_engine('sqlite:///' + filename)\n",
    "\n",
    "### create tables in 'raw' state (with '_r'), with all columns\n",
    "### will work on these in sql to create the final tables\n",
    "# add station table\n",
    "sta.to_sql('sta_r', con=engine, if_exists='replace', index=False) # convert dataframe to sqlite db, replace old version (DANGEROUS)\n",
    "obs_eq.to_sql('obs_r', con=engine, if_exists='replace', index=False)\n",
    "loc_eq.to_sql('loc_r', con=engine, if_exists='replace', index=False)\n",
    "datamap.to_sql('map_r', con=engine, if_exists='replace', index=False)\n",
    "antUW.to_sql('antUW_r', con=engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now start manipulating tables to get final usable products\n",
    "Do commands with 'engine.execute(\"sqlCommand\")'\n",
    "example:engine.execute(\"SELECT * FROM station\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
