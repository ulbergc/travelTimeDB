{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a SQLite3 database of earthquake travel-time data\n",
    "\n",
    "Files begin in input format for struct3dp inversion program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First read in the travel time data from the different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# define the directory that has the struct3dp data files\n",
    "modelname='Run_230'\n",
    "rundir='/Users/ulberg/research/MSH/crosson/Runs'\n",
    "datadir='{}/{}/Data'.format(rundir,modelname)\n",
    "\n",
    "# These file names are always associated with an inversion\n",
    "stafile='combined.sta' # the stations that recorded travel times\n",
    "eqobsfile='quakes.obs' # the travel time observations for earthquakes\n",
    "eqlocfile='quakes.loc' # the locations of earthquakes\n",
    "exobsfile='explos.obs' # the travel time observations for explosions\n",
    "exlocfile='explos.loc' # the locations of explosions\n",
    "\n",
    "sta=pd.read_csv(datadir + '/' + stafile, names=['Station','Longitude','Latitude','Depth'], sep=' ', skipinitialspace=True)\n",
    "obs_eq=pd.read_csv(datadir + '/' + eqobsfile, names=['SourceID','Station','Phase','ArrivalTime','Uncertainty'], sep=' ', skipinitialspace=True)\n",
    "loc_eq=pd.read_csv(datadir + '/' + eqlocfile, names=['SourceID','Longitude','Latitude','Depth','EventTime'], sep=' ', skipinitialspace=True)\n",
    "obs_ex=pd.read_csv(datadir + '/' + exobsfile, names=['SourceID','Station','Phase','ArrivalTime','Uncertainty'], sep=' ', skipinitialspace=True)\n",
    "loc_ex=pd.read_csv(datadir + '/' + exlocfile, names=['SourceID','Longitude','Latitude','Depth','EventTime'], sep=' ', skipinitialspace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also read in files with information relating PNSN events to Antelope events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapfile='map.map'\n",
    "antUWfile='AntUWreview.txt'\n",
    "\n",
    "mapcols=['dbname','orid','SourceID','method','Longitude-ant','Latitude-ant','Depth-ant','FullTime']\n",
    "antUWcols=['PNSNid','dbname','orid','picker','Latitude-uw','Longitude-uw','Depth-uw']\n",
    "\n",
    "datamap=pd.read_csv(datadir + '/' + mapfile, names=mapcols, sep=' ', skipinitialspace=True)\n",
    "# antUW=pd.read_csv(rundir + '/' + modelname + '/ANT/' + antUWfile, names=antUWcols, sep=' ', skipinitialspace=True)\n",
    "antUW=pd.read_csv('{}/{}/ANT/{}'.format(rundir,modelname,antUWfile), names=antUWcols, sep=' ', skipinitialspace=True, dtype={'orid': str})\n",
    "antUW['orid']=antUW['orid'].astype('object') # so that merging later will work\n",
    "antUW['PNSNid']=antUW['PNSNid'].astype('object') # so that merging later will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to relate the source id's to the antelope database so we can get a full origin time for the event. Start with the ones that were recorded on the iMUSH broadband instruments (SourceID starts with '4', '5', '6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources were organized based on the first 1 or 2 digits of the id\n",
    "# define what the initial character means\n",
    "src_antelope=['4','5','6']\n",
    "dict_ant={}\n",
    "for k in src_antelope:\n",
    "    for j in range(1,5):\n",
    "        dict_ant[k+str(j)]='201' + k + '_Q' + str(j)\n",
    "        \n",
    "src_pnsn=['9']\n",
    "src_shot=['8'] # or anything else\n",
    "\n",
    "# read in the first and second characters in the SourceID, this bit could use some cleaning\n",
    "srcID0=loc_eq['SourceID'].apply(lambda x: x[0])\n",
    "# srcID1=loc_eq['SourceID'].apply(lambda x: x[1])\n",
    "# srcID01=loc_eq['SourceID'].apply(lambda x: x[:2])\n",
    "# print('Data types: ' + str(srcID0.unique()))\n",
    "\n",
    "# is it from antelope?\n",
    "# isAnt=srcID0.apply(lambda x: x in src_antelope)\n",
    "loc_eq['dbname']=loc_eq['SourceID'].apply(lambda x: x[:2]).map(dict_ant)\n",
    "# loc_eq.dbname.value_counts() # how many events from each quarter are there?\n",
    "\n",
    "# if the source is in antelope, get the antelope orid (This could also be done with datamap, or doesn't have to be done here at all)\n",
    "loc_eq['SourceID'][loc_eq['dbname'].notnull()]\n",
    "loc_eq['orid']=loc_eq[['SourceID','dbname']].apply(lambda x: x[0][-5:-1] if pd.notnull(x[1]) else 'NaN', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the full origin time from datamap to loc_eq, plus pnsn location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceID</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Depth</th>\n",
       "      <th>EventTime</th>\n",
       "      <th>dbname</th>\n",
       "      <th>orid</th>\n",
       "      <th>FullTime</th>\n",
       "      <th>PNSNid</th>\n",
       "      <th>Latitude-uw</th>\n",
       "      <th>Longitude-uw</th>\n",
       "      <th>Depth-uw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4301394p</td>\n",
       "      <td>-122.4719</td>\n",
       "      <td>45.8537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.897</td>\n",
       "      <td>2014_Q3</td>\n",
       "      <td>1394</td>\n",
       "      <td>20140701T222626.897</td>\n",
       "      <td>60810642</td>\n",
       "      <td>45.8663</td>\n",
       "      <td>-122.4492</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4301395p</td>\n",
       "      <td>-122.6557</td>\n",
       "      <td>45.5633</td>\n",
       "      <td>15.500</td>\n",
       "      <td>41.314</td>\n",
       "      <td>2014_Q3</td>\n",
       "      <td>1395</td>\n",
       "      <td>20140703T132841.314</td>\n",
       "      <td>60058633</td>\n",
       "      <td>45.5633</td>\n",
       "      <td>-122.6557</td>\n",
       "      <td>15.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4301396p</td>\n",
       "      <td>-122.4267</td>\n",
       "      <td>46.1724</td>\n",
       "      <td>0.123</td>\n",
       "      <td>28.559</td>\n",
       "      <td>2014_Q3</td>\n",
       "      <td>1396</td>\n",
       "      <td>20140709T181928.559</td>\n",
       "      <td>60814162</td>\n",
       "      <td>46.1728</td>\n",
       "      <td>-122.4225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4301401p</td>\n",
       "      <td>-122.4558</td>\n",
       "      <td>46.3526</td>\n",
       "      <td>1.420</td>\n",
       "      <td>30.893</td>\n",
       "      <td>2014_Q3</td>\n",
       "      <td>1401</td>\n",
       "      <td>20140714T173030.893</td>\n",
       "      <td>0</td>\n",
       "      <td>46.3526</td>\n",
       "      <td>-122.4558</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4301404p</td>\n",
       "      <td>-121.9153</td>\n",
       "      <td>46.8328</td>\n",
       "      <td>10.400</td>\n",
       "      <td>52.527</td>\n",
       "      <td>2014_Q3</td>\n",
       "      <td>1404</td>\n",
       "      <td>20140715T135452.527</td>\n",
       "      <td>60818252</td>\n",
       "      <td>46.8328</td>\n",
       "      <td>-121.9153</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SourceID  Longitude  Latitude   Depth  EventTime   dbname  orid  \\\n",
       "0  4301394p  -122.4719   45.8537   0.000     26.897  2014_Q3  1394   \n",
       "1  4301395p  -122.6557   45.5633  15.500     41.314  2014_Q3  1395   \n",
       "2  4301396p  -122.4267   46.1724   0.123     28.559  2014_Q3  1396   \n",
       "3  4301401p  -122.4558   46.3526   1.420     30.893  2014_Q3  1401   \n",
       "4  4301404p  -121.9153   46.8328  10.400     52.527  2014_Q3  1404   \n",
       "\n",
       "              FullTime    PNSNid  Latitude-uw  Longitude-uw  Depth-uw  \n",
       "0  20140701T222626.897  60810642      45.8663     -122.4492       0.0  \n",
       "1  20140703T132841.314  60058633      45.5633     -122.6557      15.9  \n",
       "2  20140709T181928.559  60814162      46.1728     -122.4225       0.0  \n",
       "3  20140714T173030.893         0      46.3526     -122.4558       1.4  \n",
       "4  20140715T135452.527  60818252      46.8328     -121.9153      11.8  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=loc_eq.merge(datamap[['SourceID','dbname','orid','FullTime']],left_on=['SourceID','dbname','orid'],right_on=['SourceID','dbname','orid'],how='left')\n",
    "# df['orid']=df['orid'].astype('object')\n",
    "# df['dbname']=df['dbname'].astype('object')\n",
    "# antUW['orid']=antUW['orid'].astype('object')\n",
    "# antUW['dbname']=antUW['dbname'].astype('object')\n",
    "df2=df.merge(antUW[['PNSNid','dbname','orid','Latitude-uw','Longitude-uw','Depth-uw']],left_on=['dbname','orid'],right_on=['dbname','orid'],how='left')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: 899\n",
      "df2: 899\n",
      "loc_eq: 899\n",
      "datamap: 2177\n",
      "antUW: 449\n"
     ]
    }
   ],
   "source": [
    "## check lengths of df (merged), loc_eq (s3dp), datamap (antelope), antUW (pnsn)\n",
    "print('df: {}'.format(len(df)))\n",
    "print('df2: {}'.format(len(df2)))\n",
    "print('loc_eq: {}'.format(len(loc_eq)))\n",
    "print('datamap: {}'.format(len(datamap)))\n",
    "print('antUW: {}'.format(len(antUW)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put this into a sql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following https://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "dbdir='/Users/ulberg/Documents/GitHub/travelTimeDB/DB' # make sure this directory is created before running\n",
    "filename=dbdir + '/tt_db.sqlite'\n",
    "\n",
    "# remove database to start (DANGEROUS)\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "else:\n",
    "    print(\"The file does not exist\")\n",
    "\n",
    "# open a connection to the database\n",
    "conn=sqlite3.connect(filename)\n",
    "c=conn.cursor()\n",
    "\n",
    "tableA='station'\n",
    "fieldA1='name'\n",
    "typeA1='TEXT'\n",
    "\n",
    "fieldA2='lat'\n",
    "typeA2='REAL'\n",
    "\n",
    "# creates table and adds column names, using different methods for testing\n",
    "c.execute(\"CREATE TABLE {tn} ({cn} {ct} PRIMARY KEY)\".format(tn=tableA,cn=fieldA1,ct=typeA1))\n",
    "c.execute(\"ALTER TABLE {tn} ADD COLUMN '{cn}' {ct}\".format(tn=tableA,cn=fieldA2,ct=typeA2))\n",
    "c.execute(\"ALTER TABLE {tn} ADD COLUMN '{cn}' {ct}\".format(tn=tableA,cn='lon',ct='REAL'))\n",
    "c.execute(\"ALTER TABLE {tn} ADD COLUMN '{cn}' {ct}\".format(tn=tableA,cn='depth',ct='REAL'))\n",
    "\n",
    "# inserts one row (for string entry, need extra quotes around it - '{v0}')\n",
    "c.execute(\"INSERT INTO {tn} ({idf}, {cn1}, {cn2}, {cn3}) VALUES ('{v0}', {v1}, {v2}, {v3})\".\\\n",
    "         format(tn=tableA, idf=fieldA1, cn1='lat', cn2='lon', cn3='depth', v0=sta.iloc[0]['Station'],\\\n",
    "               v1=sta.iloc[0]['Latitude'],v2=sta.iloc[0]['Longitude'],v3=sta.iloc[0]['Depth']))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# in sqlite, use \"PRAGMA table_info('station')\" to check column names\n",
    "# SELECT * FROM station limit 100 to show first 100 entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MB05'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sta.iloc[0].Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
